Here is the complete **Operational Runbook** for your AWS FinTech App. This guide focuses on *what* happens, *how* to
deploy it, and *how* to validate it in the AWS Console.

---

### **Phase 1: Environment & Project Initialization**

* **What it does:**
  Establishes the local development environment on your Windows Subsystem for Linux (WSL) and creates the strict
  directory structure required to separate infrastructure code from application logic.
* **Deployment Steps:**

1. Open WSL Terminal.
2. Create the root directory `~/fred_fintech_app`.
3. Extract your `fred_macro_pack.tar` into the folder.
4. Create the sub-folders (`src/data`, `src/model`, `infrastructure`).
5. Install Python dependencies (`boto3`, `sagemaker`, `pandas`).


* **Runbook Validation:**
* **Action:** Run `ls -R ~/fred_fintech_app` in the terminal.
* **Success Criteria:** You see the specific file tree structure (e.g., `infrastructure/template.yaml`,
  `src/model/train.py`) without errors.

---

### **Phase 2: Infrastructure Provisioning (IaC)**

* **What it does:**
  Deploys the "AWS Skeleton" using CloudFormation. It creates the S3 Bucket for storage, the SageMaker Model Registry
  Group for versioning, the SNS Topic for emails, and the IAM Roles for security permissions.
* **Deployment Steps:**

1. Navigate to the `infrastructure` folder.
2. Run the AWS CLI command: `aws cloudformation deploy --stack-name fred-stack ...`


* **Runbook Validation:**
* **Action:** Go to **AWS Console > CloudFormation**.
* **Success Criteria:** The stack `fred-stack` shows status **CREATE_COMPLETE**.
* **Action:** Go to **AWS Console > S3**.
* **Success Criteria:** A new bucket named `fred-fintech-data-[your-account-id]` exists.

---

### **Phase 3: Data Ingestion (ETL)**

* **What it does:**
  Scans your local `fred_macro_pack` CSV files, performs necessary cleaning (like handling missing dates), and securely
  uploads the "Training Ready" dataset to the S3 bucket.
* **Deployment Steps:**

1. Update `src/config.py` with the new S3 Bucket name from Phase 2.
2. Run the ingestion script: `python3 src/data/upload_data.py`.


* **Runbook Validation:**
* **Action:** Go to **AWS Console > S3 > [Your Bucket]**.
* **Success Criteria:** Navigate to the folder `fred-data/train/`. You should see `fred_merged_data.csv` (or individual
  series files) present with a timestamp matching "just now".

---

### **Phase 4: Model Training & Registration**

* **What it does:**
  Submits a job to AWS SageMaker. SageMaker spins up a temporary instance, downloads the data, runs the XGBoost
  algorithm, saves the model artifact (`model.tar.gz`), and registers it as a "Pending" version in the Model Registry.
* **Deployment Steps:**

1. Run the pipeline orchestrator: `python3 src/model/pipeline.py`.
2. Wait 3-5 minutes for the job to complete.


* **Runbook Validation:**
* **Action:** Go to **AWS Console > SageMaker > Model Registry**.
* **Success Criteria:** Click on **FredRecessionModels**. You see **Version 1** listed.
* **Critical Check:** The status column must say **PendingManualApproval**.

---

### **Phase 5: Human Approval Loop**

* **What it does:**
  This is the "Gatekeeper" phase. The system waits for human intervention. In a fully automated setup, an email is sent.
  For this POC runbook, we simulate the approval to trigger the next step.
* **Deployment Steps:**

1. (Automated) Check your email inbox for a notification from AWS SNS (if subscribed).
2. (Manual Action) Go to the AWS Console to approve the model.


* **Runbook Validation:**
* **Action:** In **SageMaker Model Registry**, click on **Version 1**.
* **Action:** Click **Update Status** button (top right).
* **Action:** Select **Approved** and click **Save**.
* **Success Criteria:** The status changes to **Approved**.

---

### **Phase 6: Automated Deployment (Serverless)**

* **What it does:**
  An EventBridge rule detects that the model status changed to "Approved". It triggers a Lambda function that deploys a
  **SageMaker Serverless Endpoint**. This allows you to make real-time predictions without paying for idle servers.
* **Deployment Steps:**

1. *Triggered automatically by the action in Phase 5.*
2. (Alternative for POC) Run the manual deploy script if EventBridge isn't fully configured.


* **Runbook Validation:**
* **Action:** Go to **AWS Console > SageMaker > Inference > Endpoints**.
* **Success Criteria:** You see an endpoint named `Fred-Serverless-Endpoint`.
* **Status Check:** The status should eventually turn from **Creating** to **InService**.

---

### **Final End-to-End Test**

**Action:** Open a Python terminal or Jupyter Notebook.
**Command:** Use `boto3` to invoke the endpoint with a sample payload (e.g., current inflation/unemployment numbers).
**Success Criteria:** The endpoint returns a JSON response with a prediction (e.g., `0.045` representing a 4.5%
predicted unemployment rate).

**Next Step:** Would you like to proceed with executing Phase 1 (Environment Setup) now?